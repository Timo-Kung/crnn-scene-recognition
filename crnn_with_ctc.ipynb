{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "793b62e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  \n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f214c9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.5.0\n",
      "Keras Version: 2.5.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, Reshape, Bidirectional, LSTM\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "print( 'TensorFlow Version:', tf.__version__ )\n",
    "print( 'Keras Version:', tf.keras.__version__ ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1f9d48ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset directory\n",
    "train = pd.read_csv('./dataset/label/public_training_data.csv')\n",
    "valid = pd.read_csv('./dataset/label/public_validation_data.csv')\n",
    "\n",
    "data_dir_train = Path('./dataset/non_modified/public_training_data/')\n",
    "data_dir_valid = Path('./dataset/non_modified/public_validation_data/')\n",
    "train['IMAGE'] = sorted(list(map(str, list(data_dir_train.glob('*.jpg')))))\n",
    "valid['IMAGE'] = sorted(list(map(str, list(data_dir_valid.glob('*.jpg')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b6752ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               filename       label  \\\n",
      "0       oR72wdOuzdHVh3sMMmC4gMG2aG5jeub  3LN996012F   \n",
      "1        7VVWhb6tKde=x=w5x6_p=lfcTlGTBa  2FV271041L   \n",
      "2      VweTPTTYjERnFWFr3R7=YFKrj9NBT1Lg  2FV413012A   \n",
      "3       XOIg4ZPERXLzBmsdMd4XNHYH1r=Urbr  3LP563013A   \n",
      "4      jm85agd8RyzlDBl6jjnpypYIuVw7BTvb  3LR436033F   \n",
      "...                                 ...         ...   \n",
      "12062  KVbBuo6P=Pv4FsTMGW84LHFgiK2H2=nU  3LZ146042A   \n",
      "12063  mVE9bWuj9S_vFwQHEIH_e5mMZ4javQF8  2G2204042K   \n",
      "12064  glBWhEUeVftedEqw=qq=mzBrC=gwMyFs  2FM573063K   \n",
      "12065  Xv19H4GQWajfdKTsHBcJoz8RaKTVa7BR   1WG43V203   \n",
      "12066                            #NAME?  1W8935042F   \n",
      "\n",
      "                                                   IMAGE  Length  \n",
      "0      dataset\\non_modified\\public_training_data\\11gU...      10  \n",
      "1      dataset\\non_modified\\public_training_data\\11vx...      10  \n",
      "2      dataset\\non_modified\\public_training_data\\12Mv...      10  \n",
      "3      dataset\\non_modified\\public_training_data\\12kv...      10  \n",
      "4      dataset\\non_modified\\public_training_data\\12oI...      10  \n",
      "...                                                  ...     ...  \n",
      "12062  dataset\\non_modified\\public_training_data\\zzQe...      10  \n",
      "12063  dataset\\non_modified\\public_training_data\\zzZ_...      10  \n",
      "12064  dataset\\non_modified\\public_training_data\\zzfv...      10  \n",
      "12065  dataset\\non_modified\\public_training_data\\zzoE...       9  \n",
      "12066  dataset\\non_modified\\public_training_data\\zzqL...      10  \n",
      "\n",
      "[12067 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "train.dropna(axis=0,inplace=True)\n",
    "valid.dropna(axis=0,inplace=True)\n",
    "\n",
    "train = train[train['label'] != 'UNREADABLE']\n",
    "valid = valid[valid['label'] != 'UNREADABLE']\n",
    "\n",
    "train.reset_index(inplace=True,drop=True)\n",
    "valid.reset_index(inplace=True,drop=True)\n",
    "\n",
    "train['label'] = train['label'].str.upper()\n",
    "valid['label'] = valid['label'].str.upper()\n",
    "\n",
    "train['Length']=train['label'].apply(lambda x : len(str(x)))\n",
    "valid['Length']=valid['label'].apply(lambda x : len(str(x)))\n",
    "\n",
    "train = train[train['Length']<=21]\n",
    "valid = valid[valid['Length']<=21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "27068ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images found:  12067\n",
      "Number of labels found:  12067 \n",
      "\n",
      "Width of any captcha image: 1232\n",
      "Height of any captcha image: 1028\n",
      "Characters present:  {'R', '3', 'G', 'h', 'M', '9', 'C', 'c', 'E', 'Z', 'S', '=', 'T', 'm', 'i', 'P', 's', '5', 'q', '4', 'D', 'Y', 't', 'Q', 'O', 'p', 'U', 'u', 'X', 'H', 'F', 'v', 'd', 'b', 'x', 'K', 'e', '6', '_', 'J', 'I', 'k', 'f', 'w', '7', 'V', '2', '1', 'a', '8', 'L', 'j', 'l', 'n', 'g', 'B', 'o', 'W', 'z', 'r', 'y', 'N'}\n",
      "Number of unique characters:  62\n",
      "Maxium length of any captcha image:  32\n"
     ]
    }
   ],
   "source": [
    "# Get list of all the images\n",
    "Images = sorted( list(map(str, list(data_dir.glob('*.jpg')))) )\n",
    "Labels = [ img.split(os.path.sep)[-1].split(\".jpg\")[0] for img in Images ]\n",
    "Characters = set( char for label in Labels for char in label )\n",
    "Image_Height = set( tf.image.decode_png(tf.io.read_file(img_path)).shape[0] for img_path in Images )\n",
    "Image_Width = set( tf.image.decode_png(tf.io.read_file(img_path)).shape[1] for img_path in Images )\n",
    "\n",
    "# Maximum length of any captcha in the dataset\n",
    "Max_Length = max([ len(label) for label in Labels ])\n",
    "\n",
    "print( 'Number of images found: ', len(Images) )\n",
    "print( 'Number of labels found: ', len(Labels), '\\n' )\n",
    "print( 'Width of any captcha image:', next(iter(Image_Width)) )\n",
    "print( 'Height of any captcha image:', next(iter(Image_Height)) )\n",
    "print( 'Characters present: ', Characters ) \n",
    "print( 'Number of unique characters: ', len(Characters) )\n",
    "print( 'Maxium length of any captcha image: ', Max_Length )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7a6eebb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desired image dimensions where img_width % 4==0 and img_hight % 16==0 \n",
    "img_width = 256\n",
    "img_height = 64\n",
    "\n",
    "# Number of passes of the entire training set the algorithm has completed\n",
    "Epochs = 128\n",
    "\n",
    "# Number of each batch\n",
    "Batch_Size = 120\n",
    "\n",
    "# Early Stopping\n",
    "'''\n",
    "min_delta: 容忍模型進步的最小幅度\n",
    "patience: 容忍訓練無再進步時的 Epoch 次數\n",
    "'''\n",
    "Early_Stopping_Patience = 10 \n",
    "Min_Delta = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0d901a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping characters to integers\n",
    "char_to_num = layers.experimental.preprocessing.StringLookup(\n",
    "                  vocabulary=sorted(list(Characters)), num_oov_indices=0, mask_token=None )\n",
    "\n",
    "# Mapping integers back to original characters\n",
    "num_to_char = layers.experimental.preprocessing.StringLookup(\n",
    "                  vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6d3d2203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_single_sample( img_path, label ):\n",
    "    # 1. Read image\n",
    "    img = tf.io.read_file( img_path )\n",
    "    # 2. Decode and convert to grayscale\n",
    "    img = tf.io.decode_png( img, channels=1 )\n",
    "    # 3. Convert to float32 in [0, 1] range\n",
    "    img = tf.image.convert_image_dtype( img, tf.float32 )\n",
    "    # 4. Resize to the desired size\n",
    "    img = tf.image.resize( img, [img_height,img_width] )\n",
    "    # 5. Transpose the image because we want the time dimension to correspond to the width of the image, \n",
    "    #    i.e., shape = (img_weight,img_height,1).\n",
    "    img = tf.transpose( img, perm=[1,0,2] )\n",
    "    # 6. Map the characters in label to numbers\n",
    "    label = char_to_num( tf.strings.unicode_split(label, input_encoding='UTF-8') )\n",
    "    # 7. Return a dict as our model is expecting two inputs\n",
    "    return { \"Input\": img, \"Label\": label }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "52ba580c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training set: 10860\n",
      "Number of validation set: 1207\n"
     ]
    }
   ],
   "source": [
    "# Splitting dataset into training and validation sets\n",
    "def split_data( Images, Labels, train_size=0.9, shuffle=True ):\n",
    "    # 1. Get the total amount of the dataset\n",
    "    size = len( Images )\n",
    "    # 2. Make an indices array and shuffle it, if required\n",
    "    indices = np.arange( size )\n",
    "    if shuffle:\n",
    "        np.random.seed(42) \n",
    "        np.random.shuffle( indices )\n",
    "    # 3. Get the size of training samples\n",
    "    train_samples = int( size*train_size )\n",
    "    # 4. Split data into training and validation sets\n",
    "    x_train, y_train = Images[indices[:train_samples]], Labels[indices[:train_samples]]\n",
    "    x_valid, y_valid = Images[indices[train_samples:]], Labels[indices[train_samples:]]\n",
    "    return x_train, x_valid, y_train, y_valid\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = split_data( np.array(Images), np.array(Labels) )\n",
    "\n",
    "print( 'Number of training set:', len(x_train) )\n",
    "print( 'Number of validation set:', len(x_valid) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5aec9692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and valodation dataset objects\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices( (x_train,y_train) )\n",
    "train_dataset = (\n",
    "    train_dataset.map(\n",
    "        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "    .batch(Batch_Size)\n",
    "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices( (x_valid,y_valid) )\n",
    "validation_dataset = (\n",
    "    validation_dataset.map(\n",
    "        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "    .batch(Batch_Size)\n",
    "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f45cb1ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [30], [batch]: [31] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-c060196ca11f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Visualize the Training Dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    759\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 761\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    762\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    742\u001b[0m     \u001b[1;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 744\u001b[1;33m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[0;32m    745\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2725\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2726\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2727\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2728\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2729\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6895\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6896\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6897\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6898\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6899\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [30], [batch]: [31] [Op:IteratorGetNext]"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the Training Dataset\n",
    "_, ax = plt.subplots(4, 4, figsize=(15, 10))\n",
    "for batch in train_dataset.take(1):\n",
    "    images = batch[\"image\"]\n",
    "    labels = batch[\"label\"]\n",
    "    for i in range(16):\n",
    "        img = (images[i] * 255).numpy().astype(\"uint8\")\n",
    "        label = tf.strings.reduce_join(num_to_char(labels[i])).numpy().decode(\"utf-8\")\n",
    "        ax[i // 4, i % 4].imshow(img[:, :, 0].T, cmap=\"gray\")\n",
    "        ax[i // 4, i % 4].set_title(label)\n",
    "        ax[i // 4, i % 4].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb3229d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an endpoint layer for implementing CTC loss.\n",
    "class CTCLayer( layers.Layer ):\n",
    "    def __init__( self, name=None, **kwargs ):\n",
    "        super().__init__( name=name )\n",
    "        self.loss_fn = K.ctc_batch_cost\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Compute the training-time loss value and add it to the layer using `self.add_loss()`.\n",
    "        batch_len = tf.cast( tf.shape(y_true)[0], dtype='int64' )\n",
    "        input_length = tf.cast( tf.shape(y_pred)[1], dtype='int64' )\n",
    "        label_length = tf.cast( tf.shape(y_true)[1], dtype='int64' )\n",
    "        \n",
    "        input_length = input_length*tf.ones( shape=(batch_len,1), dtype='int64' )\n",
    "        label_length = label_length*tf.ones( shape=(batch_len,1), dtype='int64' )\n",
    "\n",
    "        loss = self.loss_fn( y_true, y_pred, input_length, label_length )\n",
    "        self.add_loss(loss)\n",
    "\n",
    "        # At test time, just return the computed predictions\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c30aa133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CRNN_Model_with_CTC_LOSS\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              [(None, 250, 550, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1 (Conv2D)                 (None, 250, 550, 64) 640         Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "MaxPooling_1 (MaxPooling2D)     (None, 125, 275, 64) 0           Conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Conv_2 (Conv2D)                 (None, 125, 275, 128 73856       MaxPooling_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "MaxPooling_2 (MaxPooling2D)     (None, 62, 137, 128) 0           Conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Conv_3 (Conv2D)                 (None, 62, 137, 256) 295168      MaxPooling_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "BN_1 (BatchNormalization)       (None, 62, 137, 256) 1024        Conv_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Conv_4 (Conv2D)                 (None, 62, 137, 512) 1180160     BN_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "MaxPooling_3 (MaxPooling2D)     (None, 31, 69, 512)  0           Conv_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Conv_5 (Conv2D)                 (None, 30, 68, 512)  1049088     MaxPooling_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Reshape (Reshape)               (None, 30, 34816)    0           Conv_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Dense (Dense)                   (None, 30, 64)       2228288     Reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Dropout (Dropout)               (None, 30, 64)       0           Dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "LSTM_1 (Bidirectional)          (None, 30, 512)      657408      Dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "LSTM_2 (Bidirectional)          (None, 30, 512)      1574912     LSTM_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Label (InputLayer)              [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Softmax (Dense)                 (None, 30, 63)       32319       LSTM_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "CTC_Loss (CTCLayer)             (None, 30, 63)       0           Label[0][0]                      \n",
      "                                                                 Softmax[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 7,092,863\n",
      "Trainable params: 7,092,351\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def Build_Model():\n",
    "    # Inputs to the model \n",
    "    input_tensor = Input( (img_width,img_height,1), dtype='float32', name='Input' )\n",
    "    labels = Input( shape=(None,), dtype='float32', name=\"Label\" )\n",
    "    \n",
    "    # First convolution block\n",
    "    x = Conv2D( 64, (3,3), activation='relu', padding='same', name='Conv_1' )(input_tensor)\n",
    "    x = MaxPooling2D( pool_size=(2,2), strides=2, name='MaxPooling_1' )(x)\n",
    "    \n",
    "    # Second convolution block\n",
    "    x = Conv2D( 128, (3,3), activation='relu', padding='same', name='Conv_2' )(x)\n",
    "    x = MaxPooling2D( pool_size=(2,2), strides=2, name='MaxPooling_2' )(x)\n",
    "    \n",
    "    # Third convolution block\n",
    "    x = Conv2D( 256, (3,3), activation='relu', padding='same', name='Conv_3' )(x)\n",
    "    x = BatchNormalization( name='BN_1' )(x)\n",
    "    \n",
    "    # Fourth convolution block\n",
    "    x = Conv2D( 512, (3,3), activation='relu', padding='same', name='Conv_4' )(x)  \n",
    "    x = MaxPooling2D( pool_size=(2,1), strides=2, name='MaxPooling_3' )(x)\n",
    "    \n",
    "    # Fifth convolution block\n",
    "    x = Conv2D( 512, (2,2), activation='relu', name='Conv_5' )(x)\n",
    "    \n",
    "    # Reshape accordingly before passing the output to the RNN part of the model,\n",
    "    # i.e., convert shape=(batch,feature_weight,feature_height,channel) to shape=(batch,feature_weight,channel*feature_height) \n",
    "    Conv_Shape = x.get_shape( )\n",
    "    x = Reshape( target_shape=(int(Conv_Shape[1]),int(Conv_Shape[2]*Conv_Shape[3])), name='Reshape' )(x)\n",
    "    x = Dense( 64, activation='relu', name='Dense' )(x)\n",
    "    x = Dropout( 0.25, name='Dropout' )(x)\n",
    "\n",
    "    # RNNs\n",
    "    x = Bidirectional( LSTM( 256, return_sequences=True, dropout=0.25 ), name='LSTM_1' )(x)\n",
    "    x = Bidirectional( LSTM( 256, return_sequences=True, dropout=0.25 ), name='LSTM_2' )(x)\n",
    "\n",
    "    # Output layer\n",
    "    x = Dense( len(Characters)+1, activation='softmax', name='Softmax' )(x) \n",
    "\n",
    "    # Add CTC layer for calculating CTC loss at each step\n",
    "    output = CTCLayer( name='CTC_Loss' )(labels, x)\n",
    "    \n",
    "    # Define the model\n",
    "    model = Model( inputs=[input_tensor,labels], outputs=[output], name='CRNN_Model_with_CTC_LOSS' )    \n",
    "    \n",
    "    # Compile the model and return\n",
    "    model.compile( optimizer=Adam() )\n",
    "    \n",
    "    return model\n",
    "\n",
    "CRNN = Build_Model()\n",
    "CRNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d131357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Check Point\n",
    "Check_Point = ModelCheckpoint( 'CRNN.h5',   # Filepath\n",
    "                               monitor='val_loss',\n",
    "                               save_best_only=True,\n",
    "                               verbose=1,\n",
    "                               mode='auto',\n",
    "                               save_weights_only=False,\n",
    "                               save_freq='epoch' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "477be657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add early stopping\n",
    "Early_Stopping = EarlyStopping( monitor='val_loss',\n",
    "                                min_delta=Min_Delta,\n",
    "                                patience=Early_Stopping_Patience,\n",
    "                                verbose=1,\n",
    "                                mode='auto',\n",
    "                                baseline=None,\n",
    "                                restore_best_weights=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee2b8548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "2 root error(s) found.\n  (0) Invalid argument:  Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [30], [batch]: [32]\n\t [[node IteratorGetNext (defined at <ipython-input-29-bb4eac9e3457>:1) ]]\n  (1) Invalid argument:  Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [30], [batch]: [32]\n\t [[node IteratorGetNext (defined at <ipython-input-29-bb4eac9e3457>:1) ]]\n\t [[IteratorGetNext/_7]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_85178]\n\nFunction call stack:\ntrain_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-bb4eac9e3457>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m History = CRNN.fit( train_dataset, validation_data=validation_dataset, epochs=Epochs, \n\u001b[0m\u001b[0;32m      2\u001b[0m                     callbacks=[ Check_Point, Early_Stopping ] )     \n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument:  Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [30], [batch]: [32]\n\t [[node IteratorGetNext (defined at <ipython-input-29-bb4eac9e3457>:1) ]]\n  (1) Invalid argument:  Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [30], [batch]: [32]\n\t [[node IteratorGetNext (defined at <ipython-input-29-bb4eac9e3457>:1) ]]\n\t [[IteratorGetNext/_7]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_85178]\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "History = CRNN.fit( train_dataset, validation_data=validation_dataset, epochs=Epochs, \n",
    "                    callbacks=[ Check_Point, Early_Stopping ] )     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
